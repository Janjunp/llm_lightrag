<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d5" for="edge" attr.name="keywords" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;SLOW FUSION SPATIO-TEMPORAL CONVNET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Slow Fusion spatio-temporal ConvNet uses a simpler approach and underperforms in comparison to the Two-stream model."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;IDT WITH STACKED FISHER ENCODING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"IDT with Stacked Fisher Encoding uses deep Fisher Nets and achieves better class recall on HMDB-51, still not as effective as the Two-stream model."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;SPATIO-TEMPORAL HMAX NETWORK&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Spatio-temporal HMAX network is an older method that performs poorly compared to more recent techniques like the Two-stream model."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;IMPROVED DENSE TRAJECTORIES (IDT)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"IDT is an older method used for video classification, performing decently but not as well as the proposed models."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;DEEP FISHER NET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Deep Fisher Net is a component of IDT with Stacked Fisher Encoding, providing better performance on HMDB-51."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;GENERIC ASSUMPTIONS OF CONSTANCY AND SMOOTHNESS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"These generic assumptions guide the computation of optical flow in the Temporal ConvNet."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;KAREN SIMONYAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Karen Simonyan is one of the authors contributing to the research on Two-Stream Convolutional Networks."</data>
  <data key="d2">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;ANDREW ZISSERMAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Andrew Zisserman is an author and co-contributor to the research on Two-Stream Convolutional Networks."</data>
  <data key="d2">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;CONVNET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"ConvNet (Convolutional Neural Network) is used in processing and analyzing optical flow data for video representations."&lt;SEP&gt;"ConvNet is a type of neural network architecture used for processing visual data, particularly in tasks like image classification and optical flow analysis."&lt;SEP&gt;"ConvNet is a type of neural network, specifically designed for image and video recognition tasks."&lt;SEP&gt;"ConvNet stands for Convolutional Neural Network, a deep learning model often used in image recognition tasks. The text discusses its use in both spatial and temporal contexts."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-34a54fcc59f0d516a7e1556a7de2f32b&lt;SEP&gt;chunk-cf87ec63a4b1e44cb5a04807041bf1c1&lt;SEP&gt;chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;HAND-CRAFTED FEATURES&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Hand-crafted features refer to manually engineered feature extraction methods that have been traditionally used in computer vision tasks."</data>
  <data key="d2">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;LARGE VIDEO DATASETS&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Large video datasets are mentioned as potential sources for future training, but not yet utilized extensively due to size constraints."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;CVPR&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Conference on Computer Vision and Pattern Recognition, an annual conference focused on computer vision research."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;DEEP CONVOLUTIONAL NETWORKS (CONVNETS)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Deep Convolutional Networks, also known as ConvNets, are neural network architectures used for image and video processing."</data>
  <data key="d2">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;PROC. CVPR &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Proc. CVPR (Conference on Computer Vision and Pattern Recognition) is an event where H. Wang, A. Kläser, C. Schmid, and C.-L. Liu presented their paper on 'Action recognition by dense trajectories'"</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;F. PERRONNIN, J. SÁNCHEZ, T. MENSINK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These individuals contributed to the paper on improving the Fisher kernel for large-scale image classification."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;KINEMATIC FEATURES&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Kinematic features such as divergence, curl, and shear are computed from the optical flow gradient and captured by our convolutional model."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;K. SIMONYAN, A. VEDALDI, A. ZISSERMAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These individuals contributed to the paper on deep Fisher networks for large-scale image classification."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;SPATIAL RECEPTIVE FIELD&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Spatial receptive field refers to the area that a filter covers on an image or feature map."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;IDT WITH HIGHER-DIMENSIONAL ENCODINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"IDT with Higher-Dimensional Encodings is a method that improves accuracy but falls short compared to the proposed Two-stream model."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;TRAJECTORY FEATURE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The trajectory feature is a descriptor computed by stacking displacement vectors along the trajectory, which corresponds to the trajectory stacking in the ConvNet architecture."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;K. SOOMRO, A. R. ZAMIR, M. SHAH&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These individuals developed UCF101, a dataset of 101 human action classes from videos in the wild."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;HOF AND MBH LOCAL DESCRIPTORS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"HOF (Histogram of Oriented Gradients) and MBH (Multi-Band Histogram) local descriptors are based on the histograms of orientations of optical flow or its gradient, which can be obtained using a single convolutional layer."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;TWO-STREAM MODEL (FUSION BY SVM)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Two-stream model fuses the outputs of Spatial and Temporal ConvNets using an SVM method, showing even better performance than averaging."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;G. W. TAYLOR, R. FERGUS, Y. LECUN, C. BREGLER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These researchers worked on convolutional learning of spatio-temporal features."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;GPUS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GPUs are computational hardware used in research to process data, likely in a deep learning context."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;H. WANG, C. SCHMID&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"H. Wang and C. Schmid are the authors behind multiple publications including action recognition with improved trajectories, LEAR-INRIA submission for the THUMOS workshop, evaluation of local spatiotemporal features for action recognition, and action recognition by dense trajectories."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;ILSVRC&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Large Scale Visual Recognition Challenge (ILSVRC) is a well-known challenge in computer vision."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;H. WANG, M. M. ULLAH, A. KLÄSER, I. LAPTEV, C. SCHMID, C.-L. LIU&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This group of researchers contributed to the paper on evaluating local spatiotemporal features for action recognition."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;ICCV WORKSHOP ON ACTION RECOGNITION WITH A LARGE NUMBER OF CLASSES &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This workshop was part of ICCV where H. Wang and colleagues submitted their work."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;P. WEINZAEPFEL, J. REVAUD, Z. HARCHAOUI, C. SCHMID&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"These individuals worked on DeepFlow: Large displacement optical flow with deep matching."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;UCF-101&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"UCF-101 is a benchmark dataset for video action recognition used to evaluate the performance of Two-Stream Convolutional Networks."&lt;SEP&gt;"UCF-101 is a dataset used for evaluating the performance of video classification models."&lt;SEP&gt;"UCF-101 is an action recognition benchmark dataset containing 13,000 videos, each with 180 frames on average, annotated into 101 action classes. It is used for evaluating the performance of ConvNets."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd&lt;SEP&gt;chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;M. D. ZEILER, R. FERGUS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These researchers visualized and understood convolutional networks."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;G. W. TAYLOR, R. FERGUS, Y . LECUN, C. BREGLER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These researchers worked on convolutional learning of spatio-temporal features."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;TEMPORAL CONVNET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Temporal ConvNet is a deep learning model used for action recognition by processing temporal information from video frames."&lt;SEP&gt;"Temporal ConvNet is another type of Convolutional Neural Network used for temporal processing, often involving sequence data like video frames."&lt;SEP&gt;"The Temporal ConvNet is trained on UCF and HMDB using multi-task learning with uni-directional optical flow stacking, enhancing video classification accuracy."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd&lt;SEP&gt;chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</node>
<node id="&quot;PROC. ECCV &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Proc. ECCV is a conference proceedings where several papers were presented."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;TWO-STREAM CONVOLUTIONAL NETWORKS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Two-Stream Convolutional Networks is a proposed architecture for action recognition in videos, which incorporates spatial and temporal networks."</data>
  <data key="d2">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;CAFFE TOOLBOX&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Caffe is an open-source deep learning framework developed by Berkeley AI Research (BAIR). It provides a platform for implementing and training neural networks, including ConvNets used in this text."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</node>
<node id="&quot;NIPS &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"NIPS (Neural Information Processing Systems) is an event where K. Simonyan et al. contributed their work."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;SPATIAL CONVNET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Spatial ConvNet is a type of Convolutional Neural Network used for spatial processing in image recognition tasks."&lt;SEP&gt;"Spatial ConvNet is another type of deep learning model, which processes spatial information from single frames in videos."&lt;SEP&gt;"The Spatial ConvNet is pre-trained on ILSVRC and its last layer is trained on UCF or HMDB, showing high accuracy in video classification."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd&lt;SEP&gt;chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</node>
<node id="&quot;CORR &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"CoRR is the venue where K. Soomro et al. published UCF101."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;OPTICAL FLOW DISPLACEMENT FIELDS D&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Optical flow displacement fields are used as input channels for the 96ﬁlters, capturing motion changes in images."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;ICML&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"International Conference on Machine Learning, an annual conference for machine learning research."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;PROC. ICCV &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Proc. ICCV is an event where H. Wang and colleagues presented their works on action recognition and optical flow techniques."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;96FILTERS&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"96ﬁlters are convolutional filters used in a ConvNet, each with a receptive field of 7x7 pixels and spanning 20 input channels corresponding to optical flow displacement fields."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;PROC. BMVC &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Proc. BMVC is the venue where H. Wang et al.'s work on evaluating local spatiotemporal features for action recognition was published."</data>
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</node>
<node id="&quot;NIPS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Neural Information Processing Systems Conference, a leading conference for neural networks and machine learning."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;RELU ACTIVATION FUNCTION&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"ReLU (Rectified Linear Unit) is an activation function that introduces non-linearity to the ConvNet by setting all negative values to zero and keeping positive values unchanged."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;MULTI-TASK LEARNING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Multi-task Learning is a learning framework where multiple related tasks are learned simultaneously from shared inputs, increasing the amount of training data and improving performance."&lt;SEP&gt;"Multi-task learning is a technique used for training ConvNets on multiple datasets, aiming to generalize the learned representation across different tasks."&lt;SEP&gt;"Multi-task learning is used to train the ConvNets for video classification, improving performance both alone and when fused with a spatial net."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd&lt;SEP&gt;chunk-bb08a015abf427870e566013b6eb1dd2&lt;SEP&gt;chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;LOCAL RESPONSE NORMALIZATION&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Local Response Normalization (LRN) is a technique that normalizes the activity across channels locally, helping to reduce overfitting and improve generalization."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;BMVC&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"British Machine Vision Conference, an annual conference focused on computer vision."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;TWO-STREAM MODEL (FUSION BY AVERAGING)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Two-stream model combines Spatial and Temporal ConvNets using averaging, achieving high accuracy on UCF-101 and HMDB-51."</data>
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</node>
<node id="&quot;MAX-POOLING&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Max-pooling is a downsampling technique in neural networks where the maximum value from a defined window of the feature map is selected, reducing spatial dimensions while retaining important information."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;MULTI-GPU TRAINING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Multi-GPU training refers to the use of multiple Graphics Processing Units in parallel to speed up the training process of deep learning models."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</node>
<node id="&quot;MINI-BATCH STOCHASTIC GRADIENT DESCENT WITH MOMENTUM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Mini-batch Stochastic Gradient Descent (SGD) with momentum is an optimization algorithm used for training deep neural networks, where a mix of current gradients and past gradients are used to update weights."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;ECCV&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"European Conference on Computer Vision, a major conference in the field of computer vision."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;OPENCV TOOLBOX&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"OpenCV (Open Source Computer Vision Library) is an open-source computer vision and machine learning software library that provides functions for image processing and computer vision tasks. It is mentioned as providing a GPU implementation for optical flow computation."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</node>
<node id="&quot;HMDB-51 DATASET&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"HMDB-51 dataset is another dataset used for recognizing human motion and contains 51 classes of actions, smaller than UCF-101 in terms of training set size."&lt;SEP&gt;"HMDB-51 dataset is another video benchmark with 6,800 videos and 51 action classes, often used alongside UCF-101 for comprehensive evaluation of action recognition models."&lt;SEP&gt;"HMDB-51 is a smaller video dataset with 3.7K videos, also used for training and testing the ConvNets."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-eb84389e2cb0fb2dfc96d4777be87037&lt;SEP&gt;chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;TOP-5 ERROR&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Top-5 error is the percentage of test cases that the model fails to place in its top 5 predictions."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;IMAGENET CHALLENGE DATASET&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"The ImageNet challenge dataset is used to pre-train the ConvNet on large-scale image classification tasks."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;VISUAL GEOMETRY GROUP, UNIVERSITY OF OXFORD&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Visual Geometry Group at the University of Oxford is the institution where Karen Simonyan and Andrew Zisserman are based."</data>
  <data key="d2">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;DX AND DY COMPONENTS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The dx and dy components refer to horizontal and vertical displacements of optical flow, computed by some ﬁlters to capture how motion changes with image location."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;TEMPORAL DERIVATIVES&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Temporal derivatives are computed by other ﬁlters to capture changes in motion over time."</data>
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;SINGLE-GPU TRAINING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Single-GPU training refers to the use of a single Graphics Processing Unit for training deep learning models. It is mentioned in comparison with multi-GPU training for speed improvements."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</node>
<node id="&quot;SECT. 5&quot;">
  <data key="d0">"SECTION"</data>
  <data key="d1">"Section 5 provides details on how the spatial recognition stream of the system is built using a pre-trained ConvNet on a large dataset like ImageNet."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;DROPOUT REGULARIZATION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time, which helps in making the model more robust and generalizable."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</node>
<node id="&quot;TEMPORAL STREAM CONVNET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The temporal stream ConvNet is described as an improvement over the spatial recognition stream, exploiting motion to significantly increase accuracy."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;OPTICAL FLOW STACKING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Optical flow stacking is a method of forming input for the temporal recognition stream by stacking displacement vector fields between several consecutive frames."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;SECT. 6&quot;">
  <data key="d0">"SECTION"</data>
  <data key="d1">"Section 6 discusses action classification from still frames, indicating it is part of a larger document or research paper."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;STILL FRAMES&quot;">
  <data key="d0">"DATA"</data>
  <data key="d1">"Still frames are the input data for the spatial recognition stream, used in image classification tasks."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;MOTION AND SIGNIFICATION IMPROVEMENT&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The use of motion significantly improves accuracy in the temporal recognition stream."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;OPTICAL FLOW DISPLACEMENT FIELD&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Optical flow displacement field is a technique used to describe the movement between consecutive frames explicitly."</data>
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;ACTION RECOGNITION&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Action Recognition is a task of identifying human actions within videos."</data>
  <data key="d2">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;HMDB-51&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"HMDB-51 is another action recognition benchmark dataset containing 6,800 videos with 51 action classes. Like UCF-101, it is used in conjunction with UCF-101 for evaluating the performance of ConvNets."&lt;SEP&gt;"HMDB-51 is another benchmark dataset for video action recognition utilized in evaluating the proposed architecture."&lt;SEP&gt;"HMDB-51 is another dataset used alongside UCF-101 for testing and comparison of video classification methods."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd&lt;SEP&gt;chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</node>
<node id="&quot;MEAN FLOW SUBTRACTION&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Mean flow subtraction is a method used to zero-center the network input for better exploitation of rectification nonlinearities."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;DISPLACEMENT FIELD D&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Displacement field d refers to a vector field that describes pixel locations between frames."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;UCF-101 DATASET&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"UCF-101 dataset is a collection of videos from various categories, used to train the ConvNet on optical flow analysis."&lt;SEP&gt;"UCF-101 dataset is a large-scale video benchmark containing 13,000 videos annotated into 101 action classes, used primarily for evaluating action recognition models."&lt;SEP&gt;"UCF-101 dataset is a well-known dataset for action recognition and contains 101 different classes of actions."&lt;SEP&gt;"UCF-101 is a video dataset with 9.5K videos used for action classification in experiments."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-34a54fcc59f0d516a7e1556a7de2f32b&lt;SEP&gt;chunk-eb84389e2cb0fb2dfc96d4777be87037&lt;SEP&gt;chunk-bb08a015abf427870e566013b6eb1dd2</data>
</node>
<node id="&quot;IMAGENET ILSVRC-2012&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"ImageNet ILSVRC-2012 refers to a large-scale image recognition challenge, specifically the 2012 version of ILSVRC (International Legos Visual Recognition Challenge). It is used for pre-training ConvNets and evaluating their performance."</data>
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</node>
<node id="&quot;HIDDEN LAYERS CONFIGURATION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The hidden layers configuration refers to the architecture and setup of the convolutional neural network's internal structure."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;CONVNET ARCHITECTURES&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"ConvNet architectures refer to various configurations of Convolutional Neural Networks designed for processing visual data."</data>
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</node>
<node id="&quot;ICCV&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"International Conference on Computer Vision, another major conference in the field of computer vision."</data>
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</node>
<node id="&quot;L=F5;10G&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"This represents a variable used to denote the number of stacked optical flows, ranging from 5 to 10."</data>
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</node>
<node id="&quot;SPATIO-TEMPORAL LOCAL FEATURES&quot;">
  <data key="d0">"CATEGORY"</data>
  <data key="d1">"Spatio-temporal local features are generalised by our temporal ConvNet, combining several spatio-temporal local features from optical flow."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;STACKED OPTICAL FLOWS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This refers to multiple layers of optical flow data used as input for the ConvNet architectures."</data>
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</node>
<node id="&quot;TRAJECTORY STACKING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"A technique mentioned in the text where trajectories are stacked as part of the input configuration."&lt;SEP&gt;"Trajectory stacking is an alternative motion representation that samples optical flow along motion trajectories, providing a different way to encode motion in ConvNet inputs."</data>
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037&lt;SEP&gt;chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;MEAN DISPLACEMENT SUBTRACTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This is a preprocessing step that subtracts the mean displacement from the optical flow data, used to reduce global motion effects."</data>
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</node>
<node id="&quot;BI-DIRECTIONAL OPTICAL FLOW&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"A method of generating optical flow that considers both forward and backward motion in the video frames."&lt;SEP&gt;"Bi-directional optical flow refers to computing displacement fields both forward and backward between frames, enhancing the network's ability to understand motion in videos."</data>
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037&lt;SEP&gt;chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</node>
<node id="&quot;OPTICAL FLOW&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Optical flow refers to a displacement field that describes the motion of pixels between frames."</data>
  <data key="d2">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</node>
<node id="&quot;SPATIAL RECOGNITION STREAM&quot;">
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
  <data key="d1">"The temporal stream ConvNet improves upon the spatial recognition stream by exploiting motion to enhance accuracy."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;H. WANG, R. FERGUS, C. SCHMID&quot;">
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
  <data key="d1">"H. Wang and colleagues' work on action recognition with improved trajectories and DeepFlow was presented at Proc. ICCV."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TRAINING PROCEDURE&quot;">
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
  <data key="d1">"Mini-batch SGD with momentum is used for the training procedure of the ConvNet to update weights and improve convergence."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;H. WANG, A. KLÄSER, C. SCHMID, C.-L. LIU&quot;">
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
  <data key="d1">"Their work on 'Action recognition by dense trajectories' was presented at Proc. CVPR."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ALEX&quot;">
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
  <data key="d1">"Alex's team assesses the effect of using multiple stacked optical flows."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ACTION CLASSIFICATION&quot;">
  <data key="d2">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
  <data key="d1">"The temporal stream ConvNet plays a crucial role in action classification by exploiting motion to improve accuracy."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TESTING&quot;">
  <data key="d2">chunk-bb08a015abf427870e566013b6eb1dd2</data>
  <data key="d1">"Top-5 error is evaluated during testing to assess the performance of the ConvNet model."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;RESEARCH&quot;">
  <data key="d2">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
  <data key="d1">"GPUs are donated for use in the research, indicating their importance and role in advancing the project."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;G. W. TAYLOR, R. FERGUS, Y. LECUN, C. BREGLER, P. WEINZAEPFEL, J. REVAUD, Z. HARCHAOUI, C. SCHMID&quot;">
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
  <data key="d1">"H. Wang et al.'s work builds on the foundational research by G. W. Taylor and colleagues in convolutional learning and action recognition techniques."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TECHNIQUE&quot;">
  <data key="d2">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
  <data key="d1">"Multi-task learning involves training a single model on multiple related tasks, enhancing its generalization and performance on both datasets."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;OPTICAL FLOW COMPUTATION&quot;">
  <data key="d2">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
  <data key="d1">"The off-the-shelf GPU implementation of optical flow from OpenCV is used for pre-computed optical flow in the experiments. "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TWO-STREAM MODEL&quot;">
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
  <data key="d1">"The Two-stream model is evaluated on UCF-101 dataset, showing high accuracy and outperforming other methods."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;G. W. TAYLOR, R. FERGUS, Y . LECUN, C. BREGLER, P. WEINZAEPFEL, J. REVAUD, Z. HARCHAOUI, C. SCHMID&quot;">
  <data key="d2">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
  <data key="d1">"H. Wang et al.'s work builds on the foundational research by G. W. Taylor and colleagues in convolutional learning and action recognition techniques."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SPATIO-TEMPORAL CONVNET&quot;">
  <data key="d2">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
  <data key="d1">"Multi-task learning is applied to the Temporal ConvNet, enhancing its effectiveness for optical flow-based input."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<edge source="&quot;KAREN SIMONYAN&quot;" target="&quot;VISUAL GEOMETRY GROUP, UNIVERSITY OF OXFORD&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Karen Simonyan works with the Visual Geometry Group at the University of Oxford."</data>
  <data key="d5">"affiliation, research collaboration"</data>
  <data key="d6">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</edge>
<edge source="&quot;ANDREW ZISSERMAN&quot;" target="&quot;VISUAL GEOMETRY GROUP, UNIVERSITY OF OXFORD&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Andrew Zisserman is part of the Visual Geometry Group at the University of Oxford."</data>
  <data key="d5">"affiliation, research collaboration"</data>
  <data key="d6">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;HMDB-51&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Similar to UCF-101, the ConvNet is also evaluated on HMDB-51 for action recognition, indicating its versatility in handling different datasets and scenarios."</data>
  <data key="d5">"cross-evaluation, benchmarking"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;OPTICAL FLOW&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Optical flow data is input into the ConvNet for processing and analyzing motion within video frames."</data>
  <data key="d5">"data input, analysis process"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;UCF-101 DATASET&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The UCF-101 dataset provides training data for the ConvNet to learn from." "</data>
  <data key="d5">"training data, model learning"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;LOCAL RESPONSE NORMALIZATION&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The ConvNet architecture incorporates local response normalization to improve generalization and reduce overfitting by normalizing activity across channels locally."</data>
  <data key="d5">"normalization technique, improvement"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;IMAGENET CHALLENGE DATASET&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The ConvNet is pre-trained on the ImageNet challenge dataset for image classification tasks."</data>
  <data key="d5">"pre-training, large-scale data"</data>
  <data key="d6">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;HIDDEN LAYERS CONFIGURATION&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The hidden layers configuration remains largely the same as that used in the spatial ConvNet and forms part of the temporal ConvNet architecture." "</data>
  <data key="d5">"architecture design, consistency"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;UCF-101&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The ConvNet is evaluated and trained using UCF-101 for action recognition, showcasing its application in real-world scenarios of video understanding."</data>
  <data key="d5">"training, evaluation"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;HOF AND MBH LOCAL DESCRIPTORS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The HOF and MBH local descriptors are captured by our convolutional model through the use of a single convolutional layer." "</data>
  <data key="d5">"descriptor extraction, feature learning"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;96FILTERS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The ConvNet architecture includes 96ﬁlters, which are part of its layer configuration for processing visual data."</data>
  <data key="d5">"filter inclusion, network architecture"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;CAFFE TOOLBOX&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The implementation of ConvNets is derived from the Caffe toolbox, suggesting a dependency on this framework for model training and experimentation. "</data>
  <data key="d5">"implementation, dependency"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;CONVNET&quot;" target="&quot;IMAGENET ILSVRC-2012&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ConvNet undergoes pre-training on ImageNet ILSVRC-2012 before fine-tuning on UCF-101, highlighting the use of transfer learning from large-scale datasets to specialized ones."</data>
  <data key="d5">"transfer learning, pre-training"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;HAND-CRAFTED FEATURES&quot;" target="&quot;MULTI-TASK LEARNING&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Multi-task Learning can leverage hand-crafted features to increase the amount of training data and improve performance on both tasks simultaneously."</data>
  <data key="d5">"feature enhancement, data augmentation"</data>
  <data key="d6">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</edge>
<edge source="&quot;CVPR&quot;" target="&quot;BMVC&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both focus on computer vision and pattern recognition, though CVPR is broader in scope."</data>
  <data key="d5">&lt;"conferences, computer vision"</data>
  <data key="d6">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</edge>
<edge source="&quot;CVPR&quot;" target="&quot;ILSVRC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both are well-known conferences in the field of computer vision and pattern recognition."</data>
  <data key="d5">&lt;"conferences, computer vision"</data>
  <data key="d6">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</edge>
<edge source="&quot;DEEP CONVOLUTIONAL NETWORKS (CONVNETS)&quot;" target="&quot;TWO-STREAM CONVOLUTIONAL NETWORKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The proposed Two-Stream architecture builds upon the state-of-the-art Deep Convolutional Networks for still image representation."</data>
  <data key="d5">"framework extension, architectural basis"</data>
  <data key="d6">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</edge>
<edge source="&quot;PROC. CVPR &quot;" target="&quot;H. WANG, A. KLÄSER, C. SCHMID, C.-L. LIU&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Their work on 'Action recognition by dense trajectories' was presented at Proc. CVPR."</data>
  <data key="d5">"event participation, research contribution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;F. PERRONNIN, J. SÁNCHEZ, T. MENSINK&quot;" target="&quot;G. W. TAYLOR, R. FERGUS, Y. LECUN, C. BREGLER&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both groups work on image classification and feature learning techniques, which are foundational to the broader field of action recognition."</data>
  <data key="d5">"field overlap, knowledge sharing"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;F. PERRONNIN, J. SÁNCHEZ, T. MENSINK&quot;" target="&quot;PROC. ECCV &quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Their paper on improving the Fisher kernel for large-scale image classification was presented at Proc. ECCV."</data>
  <data key="d5">"conference submission, research contribution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;F. PERRONNIN, J. SÁNCHEZ, T. MENSINK&quot;" target="&quot;G. W. TAYLOR, R. FERGUS, Y . LECUN, C. BREGLER&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both groups work on image classification and feature learning techniques, which are foundational to the broader field of action recognition."</data>
  <data key="d5">"field overlap, knowledge sharing"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;KINEMATIC FEATURES&quot;" target="&quot;TEMPORAL CONVNET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Kinematic features such as divergence, curl, and shear are computed from the optical flow gradient and can be captured by our convolutional model." "</data>
  <data key="d5">"feature computation, model capability"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;K. SIMONYAN, A. VEDALDI, A. ZISSERMAN&quot;" target="&quot;NIPS &quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Their work on deep Fisher networks for large-scale image classification was presented at NIPS."</data>
  <data key="d5">"event participation, research collaboration"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;TRAJECTORY FEATURE&quot;" target="&quot;TEMPORAL CONVNET&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The trajectory feature is learned through stack displacement vectors along the trajectory in the temporal ConvNet architecture." "</data>
  <data key="d5">"feature learning, trajectory stacking"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;K. SOOMRO, A. R. ZAMIR, M. SHAH&quot;" target="&quot;CORR &quot;">
  <data key="d3">18.0</data>
  <data key="d4">"UCF101: A dataset of 101 human actions classes from videos in the wild was published on CoRR."</data>
  <data key="d5">"dataset publication, research contribution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;K. SOOMRO, A. R. ZAMIR, M. SHAH&quot;" target="&quot;H. WANG, C. SCHMID&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"H. Wang and C. Schmid's work on action recognition involves the use of datasets like UCF101 created by K. Soomro et al., indicating a dependency on existing data sources."</data>
  <data key="d5">"dataset dependency, research collaboration"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;G. W. TAYLOR, R. FERGUS, Y. LECUN, C. BREGLER&quot;" target="&quot;PROC. ECCV &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Their work on convolutional learning of spatio-temporal features was presented at Proc. ECCV."</data>
  <data key="d5">"conference submission, research contribution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;GPUS&quot;" target="&quot;RESEARCH&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"GPUs are donated for use in the research, indicating their importance and role in advancing the project."</data>
  <data key="d5">&lt;"donation, research materials"</data>
  <data key="d6">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</edge>
<edge source="&quot;H. WANG, C. SCHMID&quot;" target="&quot;P. WEINZAEPFEL, J. REVAUD, Z. HARCHAOUI, C. SCHMID&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both groups contribute to the field of optical flow and deep matching techniques."</data>
  <data key="d5">"methodology overlap, technique evolution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;ILSVRC&quot;" target="&quot;ECCV&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both are major annual conferences in the field of computer vision."</data>
  <data key="d5">&lt;"conferences, computer vision"</data>
  <data key="d6">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</edge>
<edge source="&quot;H. WANG, M. M. ULLAH, A. KLÄSER, I. LAPTEV, C. SCHMID, C.-L. LIU&quot;" target="&quot;PROC. BMVC &quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Their work on evaluating local spatiotemporal features for action recognition was published at Proc. BMVC."</data>
  <data key="d5">"conference submission, research contribution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;H. WANG, M. M. ULLAH, A. KLÄSER, I. LAPTEV, C. SCHMID, C.-L. LIU&quot;" target="&quot;G. W. TAYLOR, R. FERGUS, Y. LECUN, C. BREGLER, P. WEINZAEPFEL, J. REVAUD, Z. HARCHAOUI, C. SCHMID&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"H. Wang et al.'s work builds on the foundational research by G. W. Taylor and colleagues in convolutional learning and action recognition techniques."</data>
  <data key="d5">"research foundation, building upon existing work"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;H. WANG, M. M. ULLAH, A. KLÄSER, I. LAPTEV, C. SCHMID, C.-L. LIU&quot;" target="&quot;G. W. TAYLOR, R. FERGUS, Y . LECUN, C. BREGLER, P. WEINZAEPFEL, J. REVAUD, Z. HARCHAOUI, C. SCHMID&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"H. Wang et al.'s work builds on the foundational research by G. W. Taylor and colleagues in convolutional learning and action recognition techniques."</data>
  <data key="d5">"research foundation, building upon existing work"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;P. WEINZAEPFEL, J. REVAUD, Z. HARCHAOUI, C. SCHMID&quot;" target="&quot;PROC. ICCV &quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Their work on DeepFlow: Large displacement optical flow with deep matching was presented at Proc. ICCV."</data>
  <data key="d5">"event participation, research collaboration"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;UCF-101&quot;" target="&quot;TWO-STREAM MODEL&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The Two-stream model is evaluated on UCF-101 dataset, showing high accuracy and outperforming other methods."</data>
  <data key="d5">"evaluation, superior performance"</data>
  <data key="d6">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</edge>
<edge source="&quot;UCF-101&quot;" target="&quot;TWO-STREAM CONVOLUTIONAL NETWORKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Two-Stream Convolutional Networks are evaluated on UCF-101, showing competitive performance with state-of-the-art methods."</data>
  <data key="d5">"performance evaluation, benchmark use"</data>
  <data key="d6">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</edge>
<edge source="&quot;M. D. ZEILER, R. FERGUS&quot;" target="&quot;CORR &quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Their paper 'Visualizing and understanding convolutional networks' was published on CoRR."</data>
  <data key="d5">"dataset publication, research contribution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;G. W. TAYLOR, R. FERGUS, Y . LECUN, C. BREGLER&quot;" target="&quot;PROC. ECCV &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Their work on convolutional learning of spatio-temporal features was presented at Proc. ECCV."</data>
  <data key="d5">"conference submission, research contribution"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;TEMPORAL CONVNET&quot;" target="&quot;UCF-101 DATASET&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"Temporal ConvNets are trained on the UCF-101 dataset to assess their effectiveness in action recognition tasks."&lt;SEP&gt;"Temporal ConvNets are trained on the UCF-101 dataset, which is a significant source of training data for them."</data>
  <data key="d5">"evaluation, training"&lt;SEP&gt;&lt;"training, dataset usage"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f&lt;SEP&gt;chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
<edge source="&quot;TEMPORAL CONVNET&quot;" target="&quot;MULTI-TASK LEARNING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Multi-task learning is used for training the Temporal ConvNet, enhancing its effectiveness in video classification."</data>
  <data key="d5">"training method, improved accuracy"</data>
  <data key="d6">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</edge>
<edge source="&quot;TEMPORAL CONVNET&quot;" target="&quot;STACKED OPTICAL FLOWS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The temporal ConvNet is trained using the input configurations that include multiple stacked optical flows."</data>
  <data key="d5">&lt;"training, configuration"</data>
  <data key="d6">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
<edge source="&quot;TEMPORAL CONVNET&quot;" target="&quot;DROPOUT REGULARIZATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Higher dropout regularization is used in temporal ConvNets to avoid overfitting and improve generalization during training. "</data>
  <data key="d5">"regularization, prevention of overfitting"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;TEMPORAL CONVNET&quot;" target="&quot;HMDB-51 DATASET&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Temporal ConvNets are evaluated using HMDB-51 dataset, which is smaller but complementary to UCF-101 in terms of action classes."</data>
  <data key="d5">&lt;"evaluation, dataset usage"</data>
  <data key="d6">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
<edge source="&quot;TEMPORAL CONVNET&quot;" target="&quot;SPATIAL CONVNET&quot;">
  <data key="d3">27.0</data>
  <data key="d4">"The Spatial and Temporal ConvNets are combined using averaging or SVM methods to improve overall performance in video classification."&lt;SEP&gt;"The Spatial and Temporal ConvNets are combined using averaging or SVM to achieve better results in video classification."&lt;SEP&gt;"The Spatial and Temporal ConvNets are part of a two-stream model that shows superior performance on UCF-101 and HMDB-51."</data>
  <data key="d5">"fusion method, enhanced accuracy"&lt;SEP&gt;"fusion method, enhanced performance"&lt;SEP&gt;"integration, enhanced performance"</data>
  <data key="d6">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</edge>
<edge source="&quot;TEMPORAL CONVNET&quot;" target="&quot;SPATIO-TEMPORAL LOCAL FEATURES&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Our temporal ConvNet generalises spatio-temporal local features from optical flow data." "</data>
  <data key="d5">"feature extraction, model capability"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;TWO-STREAM CONVOLUTIONAL NETWORKS&quot;" target="&quot;HMDB-51&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The proposed Two-Stream architecture is also tested against HMDB-51, demonstrating superior results compared to previous attempts."</data>
  <data key="d5">"benchmark comparison, performance improvement"</data>
  <data key="d6">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</edge>
<edge source="&quot;TWO-STREAM CONVOLUTIONAL NETWORKS&quot;" target="&quot;ACTION RECOGNITION&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The Two-Stream Convolutional Networks architecture is designed specifically for action recognition in videos."</data>
  <data key="d5">"application domain, task focus"</data>
  <data key="d6">chunk-8c67a225b7a1ed3a8ac1fd7cde9eccc1</data>
</edge>
<edge source="&quot;SPATIAL CONVNET&quot;" target="&quot;MULTI-GPU TRAINING&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The spatial ConvNet benefits from multi-GPU training for faster training times and better performance. "</data>
  <data key="d5">"training speed, performance improvement"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;SPATIAL CONVNET&quot;" target="&quot;MULTI-TASK LEARNING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Multi-task learning is applied to the training of the Spatial ConvNet, improving its performance."</data>
  <data key="d5">"training method, enhanced performance"</data>
  <data key="d6">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</edge>
<edge source="&quot;SPATIAL CONVNET&quot;" target="&quot;UCF-101 DATASET&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"Spatial ConvNets are also trained on the UCF-101 dataset but perform differently from Temporal ConvNets in terms of accuracy."&lt;SEP&gt;"The spatial ConvNet is trained and evaluated on the UCF-101 dataset."</data>
  <data key="d5">&lt;"training, dataset usage"&lt;SEP&gt;&lt;"training, evaluation"</data>
  <data key="d6">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
<edge source="&quot;SPATIAL CONVNET&quot;" target="&quot;IMAGENET ILSVRC-2012&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The spatial ConvNet is pre-trained on ImageNet ILSVRC-2012 before fine-tuning on UCF-101, highlighting the transfer learning approach."</data>
  <data key="d5">"pre-training, transfer learning"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;OPTICAL FLOW DISPLACEMENT FIELDS D&quot;" target="&quot;96FILTERS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The 96ﬁlters are trained on optical flow displacement fields to compute spatial derivatives of motion."</data>
  <data key="d5">"motion capture, convolutional filters"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;ICML&quot;" target="&quot;NIPS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both are leading conferences for machine learning research."</data>
  <data key="d5">&lt;"conferences, machine learning"</data>
  <data key="d6">chunk-667a0a2b5957ead16915a7cfad43c8de</data>
</edge>
<edge source="&quot;PROC. ICCV &quot;" target="&quot;H. WANG, R. FERGUS, C. SCHMID&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"H. Wang and colleagues' work on action recognition with improved trajectories and DeepFlow was presented at Proc. ICCV."</data>
  <data key="d5">"event participation, research collaboration"</data>
  <data key="d6">chunk-640259871e7614dd8e5d3e2cd2bf19ea</data>
</edge>
<edge source="&quot;96FILTERS&quot;" target="&quot;TEMPORAL DERIVATIVES&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Some ﬁlters compute temporal derivatives to capture changes in motion over time, alongside spatial derivative computations."</data>
  <data key="d5">"motion change, temporal analysis"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;RELU ACTIVATION FUNCTION&quot;" target="&quot;MAX-POOLING&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"ReLU activation is often used in conjunction with max-pooling to introduce non-linearity and reduce spatial dimensions respectively."</data>
  <data key="d5">"activation, downsampling"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;MULTI-TASK LEARNING&quot;" target="&quot;UCF-101 DATASET&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Multi-task learning is applied by combining the training from UCF-101 and HMDB-51 datasets to improve generalization across tasks."</data>
  <data key="d5">"dataset combination, cross-task learning"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;MULTI-TASK LEARNING&quot;" target="&quot;HMDB-51 DATASET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Multi-task learning is used on the HMDB-51 dataset for training and testing ConvNets, aiding in learning generalized representations across tasks."</data>
  <data key="d5">"cross-task learning, dataset utilization"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;MULTI-TASK LEARNING&quot;" target="&quot;TECHNIQUE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Multi-task learning involves training a single model on multiple related tasks, enhancing its generalization and performance on both datasets."</data>
  <data key="d5">&lt;"training technique, multi-tasking"</data>
  <data key="d6">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
<edge source="&quot;MULTI-TASK LEARNING&quot;" target="&quot;SPATIO-TEMPORAL CONVNET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Multi-task learning is applied to the Temporal ConvNet, enhancing its effectiveness for optical flow-based input."</data>
  <data key="d5">"training method, improved accuracy"</data>
  <data key="d6">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</edge>
<edge source="&quot;MULTI-GPU TRAINING&quot;" target="&quot;SINGLE-GPU TRAINING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Multi-GPU training offers a 3:2 times speed-up over single-GPU training, indicating its superiority for faster model training. "</data>
  <data key="d5">"speed improvement, comparison"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;MINI-BATCH STOCHASTIC GRADIENT DESCENT WITH MOMENTUM&quot;" target="&quot;TRAINING PROCEDURE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Mini-batch SGD with momentum is used for the training procedure of the ConvNet to update weights and improve convergence."</data>
  <data key="d5">"optimization algorithm, weight update"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;OPENCV TOOLBOX&quot;" target="&quot;OPTICAL FLOW COMPUTATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The off-the-shelf GPU implementation of optical flow from OpenCV is used for pre-computed optical flow in the experiments. "</data>
  <data key="d5">"implementation, dependency"</data>
  <data key="d6">chunk-e09f3481cc62389e84f21a06f6eb3d7f</data>
</edge>
<edge source="&quot;TOP-5 ERROR&quot;" target="&quot;TESTING&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Top-5 error is evaluated during testing to assess the performance of the ConvNet model."</data>
  <data key="d5">"performance evaluation, test metric"</data>
  <data key="d6">chunk-bb08a015abf427870e566013b6eb1dd2</data>
</edge>
<edge source="&quot;TEMPORAL STREAM CONVNET&quot;" target="&quot;SPATIAL RECOGNITION STREAM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The temporal stream ConvNet improves upon the spatial recognition stream by exploiting motion to enhance accuracy."</data>
  <data key="d5">"improvement, motion exploitation"</data>
  <data key="d6">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</edge>
<edge source="&quot;TEMPORAL STREAM CONVNET&quot;" target="&quot;ACTION CLASSIFICATION&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The temporal stream ConvNet plays a crucial role in action classification by exploiting motion to improve accuracy."</data>
  <data key="d5">"classification, improvement"</data>
  <data key="d6">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</edge>
<edge source="&quot;OPTICAL FLOW STACKING&quot;" target="&quot;TRAJECTORY STACKING&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Optical flow stacking and trajectory stacking are different techniques for encoding motion in ConvNet inputs, each with its own method of handling optical flow data."</data>
  <data key="d5">"motion encoding, technique comparison"</data>
  <data key="d6">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</edge>
<edge source="&quot;OPTICAL FLOW STACKING&quot;" target="&quot;BI-DIRECTIONAL OPTICAL FLOW&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Bi-directional optical flow is an extension to the forward optical flow used in stacking techniques, providing more comprehensive motion understanding."</data>
  <data key="d5">"motion representation, enhancement"</data>
  <data key="d6">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</edge>
<edge source="&quot;STILL FRAMES&quot;" target="&quot;SPATIAL RECOGNITION STREAM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Still frames are the input data for the spatial recognition stream, which is part of the action classification process."</data>
  <data key="d5">"data input, recognition stream"</data>
  <data key="d6">chunk-cf87ec63a4b1e44cb5a04807041bf1c1</data>
</edge>
<edge source="&quot;HMDB-51&quot;" target="&quot;TWO-STREAM MODEL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Two-stream model is also evaluated on HMDB-51 dataset, demonstrating comparable results to state-of-the-art models."</data>
  <data key="d5">"evaluation, competitive performance"</data>
  <data key="d6">chunk-86f4b10d1cdb4e6ca763c0d4de0db9cd</data>
</edge>
<edge source="&quot;MEAN FLOW SUBTRACTION&quot;" target="&quot;DISPLACEMENT FIELD D&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The mean vector of each displacement field d is subtracted during the mean flow subtraction process." "</data>
  <data key="d5">"data processing, technique application"</data>
  <data key="d6">chunk-34a54fcc59f0d516a7e1556a7de2f32b</data>
</edge>
<edge source="&quot;STACKED OPTICAL FLOWS&quot;" target="&quot;MEAN DISPLACEMENT SUBTRACTION&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both techniques are mentioned as part of the input configurations."</data>
  <data key="d5">&lt;"complementary, preprocessing"</data>
  <data key="d6">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
<edge source="&quot;STACKED OPTICAL FLOWS&quot;" target="&quot;ALEX&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Alex's team assesses the effect of using multiple stacked optical flows."</data>
  <data key="d5">&lt;"assessment, input configuration"</data>
  <data key="d6">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
<edge source="&quot;BI-DIRECTIONAL OPTICAL FLOW&quot;" target="&quot;ALEX&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The team also uses the bi-directional optical flow in their analysis."</data>
  <data key="d5">&lt;"evaluation, alternative method"</data>
  <data key="d6">chunk-eb84389e2cb0fb2dfc96d4777be87037</data>
</edge>
</graph></graphml>