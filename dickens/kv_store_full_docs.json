{
  "doc-70a8fa8c1103b79fb5dc9ca2ed8bb7d2": {
    "content": "Two-Stream Convolutional Networks\nfor Action Recognition in Videos\nKaren Simonyan Andrew Zisserman\nVisual Geometry Group, University of Oxford\nfkaren,azg@robots.ox.ac.uk\nAbstract\nWe investigate architectures of discriminatively trained deep Convolutional Net-\nworks (ConvNets) for action recognition in video. The challenge is to capture\nthe complementary information on appearance from still frames and motion be-\ntween frames. We also aim to generalise the best performing hand-crafted features\nwithin a data-driven learning framework.\nOur contribution is three-fold. First, we propose a two-stream ConvNet architec-\nture which incorporates spatial and temporal networks. Second, we demonstrate\nthat a ConvNet trained on multi-frame dense optical Ô¨Çow is able to achieve very\ngood performance in spite of limited training data. Finally, we show that multi-\ntask learning, applied to two different action classiÔ¨Åcation datasets, can be used to\nincrease the amount of training data and improve the performance on both.\nOur architecture is trained and evaluated on the standard video actions bench-\nmarks of UCF-101 and HMDB-51, where it is competitive with the state of the\nart. It also exceeds by a large margin previous attempts to use deep nets for video\nclassiÔ¨Åcation.\n1 Introduction\nRecognition of human actions in videos is a challenging task which has received a signiÔ¨Åcant amount\nof attention in the research community [11, 14, 17, 26]. Compared to still image classiÔ¨Åcation, the\ntemporal component of videos provides an additional (and important) clue for recognition, as a\nnumber of actions can be reliably recognised based on the motion information. Additionally, video\nprovides natural data augmentation (jittering) for single image (video frame) classiÔ¨Åcation.\nIn this work, we aim at extending deep Convolutional Networks (ConvNets) [19], a state-of-the-\nart still image representation [15], to action recognition in video data. This task has recently been\naddressed in [14] by using stacked video frames as input to the network, but the results were signif-\nicantly worse than those of the best hand-crafted shallow representations [20, 26]. We investigate\na different architecture based on two separate recognition streams (spatial and temporal), which\nare then combined by late fusion. The spatial stream performs action recognition from still video\nframes, whilst the temporal stream is trained to recognise action from motion in the form of dense\noptical Ô¨Çow. Both streams are implemented as ConvNets. Decoupling the spatial and temporal nets\nalso allows us to exploit the availability of large amounts of annotated image data by pre-training\nthe spatial net on the ImageNet challenge dataset [1]. Our proposed architecture is related to the\ntwo-streams hypothesis [9], according to which the human visual cortex contains two pathways: the\nventral stream (which performs object recognition) and the dorsal stream (which recognises motion);\nthough we do not investigate this connection any further here.\nThe rest of the paper is organised as follows. In Sect. 1.1 we review the related work on action\nrecognition using both shallow and deep architectures. In Sect. 2 we introduce the two-stream\narchitecture and specify the Spatial ConvNet. Sect. 3 introduces the Temporal ConvNet and in\nparticular how it generalizes the previous architectures reviewed in Sect. 1.1. A mult-task learning\nframework is developed in Sect. 4 in order to allow effortless combination of training data over\n1arXiv:1406.2199v2  [cs.CV]  12 Nov 2014multiple datasets. Implementation details are given in Sect. 5, and the performance is evaluated\nin Sect. 6 and compared to the state of the art. Our experiments on two challenging datasets (UCF-\n101 [24] and HMDB-51 [16]) show that the two recognition streams are complementary, and our\ndeep architecture signiÔ¨Åcantly outperforms that of [14] and is competitive with the state of the art\nshallow representations [20, 21, 26] in spite of being trained on relatively small datasets.\n1.1 Related work\nVideo recognition research has been largely driven by the advances in image recognition methods,\nwhich were often adapted and extended to deal with video data. A large family of video action\nrecognition methods is based on shallow high-dimensional encodings of local spatio-temporal fea-\ntures. For instance, the algorithm of [17] consists in detecting sparse spatio-temporal interest points,\nwhich are then described using local spatio-temporal features: Histogram of Oriented Gradients\n(HOG) [7] and Histogram of Optical Flow (HOF). The features are then encoded into the Bag Of\nFeatures (BoF) representation, which is pooled over several spatio-temporal grids (similarly to spa-\ntial pyramid pooling) and combined with an SVM classiÔ¨Åer. In a later work [28], it was shown that\ndense sampling of local features outperforms sparse interest points.\nInstead of computing local video features over spatio-temporal cuboids, state-of-the-art shallow\nvideo representations [20, 21, 26] make use of dense point trajectories. The approach, Ô¨Årst in-\ntroduced in [29], consists in adjusting local descriptor support regions, so that they follow dense\ntrajectories, computed using optical Ô¨Çow. The best performance in the trajectory-based pipeline\nwas achieved by the Motion Boundary Histogram (MBH) [8], which is a gradient-based feature,\nseparately computed on the horizontal and vertical components of optical Ô¨Çow. A combination of\nseveral features was shown to further boost the accuracy. Recent improvements of trajectory-based\nhand-crafted representations include compensation of global (camera) motion [10, 16, 26], and the\nuse of the Fisher vector encoding [22] (in [26]) or its deeper variant [23] (in [21]).\nThere has also been a number of attempts to develop a deep architecture for video recognition. In\nthe majority of these works, the input to the network is a stack of consecutive video frames, so the\nmodel is expected to implicitly learn spatio-temporal motion-dependent features in the Ô¨Årst layers,\nwhich can be a difÔ¨Åcult task. In [11], an HMAX architecture for video recognition was proposed\nwith pre-deÔ¨Åned spatio-temporal Ô¨Ålters in the Ô¨Årst layer. Later, it was combined [16] with a spatial\nHMAX model, thus forming spatial (ventral-like) and temporal (dorsal-like) recognition streams.\nUnlike our work, however, the streams were implemented as hand-crafted and rather shallow (3-\nlayer) HMAX models. In [4, 18, 25], a convolutional RBM and ISA were used for unsupervised\nlearning of spatio-temporal features, which were then plugged into a discriminative model for action\nclassiÔ¨Åcation. Discriminative end-to-end learning of video ConvNets has been addressed in [12]\nand, more recently, in [14], who compared several ConvNet architectures for action recognition.\nTraining was carried out on a very large Sports-1M dataset, comprising 1.1M YouTube videos of\nsports activities. Interestingly, [14] found that a network, operating on individual video frames,\nperforms similarly to the networks, whose input is a stack of frames. This might indicate that\nthe learnt spatio-temporal features do not capture the motion well. The learnt representation, Ô¨Åne-\ntuned on the UCF-101 dataset, turned out to be 20% less accurate than hand-crafted state-of-the-art\ntrajectory-based representation [20, 27].\nOur temporal stream ConvNet operates on multiple-frame dense optical Ô¨Çow, which is typically\ncomputed in an energy minimisation framework by solving for a displacement Ô¨Åeld (typically at\nmultiple image scales). We used a popular method of [2], which formulates the energy based on\nconstancy assumptions for intensity and its gradient, as well as smoothness of the displacement Ô¨Åeld.\nRecently, [30] proposed an image patch matching scheme, which is reminiscent of deep ConvNets,\nbut does not incorporate learning.\n2 Two-stream architecture for video recognition\nVideo can naturally be decomposed into spatial and temporal components. The spatial part, in the\nform of individual frame appearance, carries information about scenes and objects depicted in the\nvideo. The temporal part, in the form of motion across the frames, conveys the movement of the\nobserver (the camera) and the objects. We devise our video recognition architecture accordingly,\ndividing it into two streams, as shown in Fig. 1. Each stream is implemented using a deep ConvNet,\nsoftmax scores of which are combined by late fusion. We consider two fusion methods: averaging\nand training a multi-class linear SVM [6] on stacked L2-normalised softmax scores as features.\n2conv1  \n7x7x96  \nstride 2  \nnorm.  \npool 2x2  conv2  \n5x5x256  \nstride 2  \nnorm.  \npool 2x2  conv3  \n3x3x512  \nstride 1  conv4  \n3x3x512  \nstride 1  conv5  \n3x3x512  \nstride 1  \npool 2x2  full6 \n4096  \ndropout  full7 \n2048  \ndropout  softmax  \n \nconv1  \n7x7x96  \nstride 2  \nnorm.  \npool 2x2  conv2  \n5x5x256  \nstride 2  \npool 2x2  conv3  \n3x3x512  \nstride 1  conv4  \n3x3x512  \nstride 1  conv5  \n3x3x512  \nstride 1  \npool 2x2  full6 \n4096  \ndropout  full7 \n2048  \ndropout  softmax  \n Spatial stream ConvNet  \nTemporal stream ConvNet  \nsingle frame  \ninput  \nvideo  \nmulti -frame  \noptical flow  class \nscore  \nfusion  \nFigure 1: Two-stream architecture for video classiÔ¨Åcation.\nSpatial stream ConvNet operates on individual video frames, effectively performing action recog-\nnition from still images. The static appearance by itself is a useful clue, since some actions are\nstrongly associated with particular objects. In fact, as will be shown in Sect. 6, action classiÔ¨Åcation\nfrom still frames (the spatial recognition stream) is fairly competitive on its own. Since a spatial\nConvNet is essentially an image classiÔ¨Åcation architecture, we can build upon the recent advances\nin large-scale image recognition methods [15], and pre-train the network on a large image classiÔ¨Åca-\ntion dataset, such as the ImageNet challenge dataset. The details are presented in Sect. 5. Next, we\ndescribe the temporal stream ConvNet, which exploits motion and signiÔ¨Åcantly improves accuracy.\n3 Optical Ô¨Çow ConvNets\nIn this section, we describe a ConvNet model, which forms the temporal recognition stream of our\narchitecture (Sect. 2). Unlike the ConvNet models, reviewed in Sect. 1.1, the input to our model is\nformed by stacking optical Ô¨Çow displacement Ô¨Åelds between several consecutive frames. Such input\nexplicitly describes the motion between video frames, which makes the recognition easier, as the\nnetwork does not need to estimate motion implicitly. We consider several variations of the optical\nÔ¨Çow-based input, which we describe below.\n(a)\n (b)\n (c)\n (d)\n (e)\nFigure 2: Optical Ô¨Çow. (a),(b): a pair of consecutive video frames with the area around a mov-\ning hand outlined with a cyan rectangle. (c): a close-up of dense optical Ô¨Çow in the outlined area;\n(d): horizontal component dxof the displacement vector Ô¨Åeld (higher intensity corresponds to pos-\nitive values, lower intensity to negative values). (e): vertical component dy. Note how (d) and (e)\nhighlight the moving hand and bow. The input to a ConvNet contains multiple Ô¨Çows (Sect. 3.1).\n3.1 ConvNet input conÔ¨Ågurations\nOptical Ô¨Çow stacking. A dense optical Ô¨Çow can be seen as a set of displacement vector Ô¨Åelds dt\nbetween the pairs of consecutive frames tandt+ 1. By dt(u;v)we denote the displacement vector\nat the point (u;v)in framet, which moves the point to the corresponding point in the following\nframet+ 1. The horizontal and vertical components of the vector Ô¨Åeld, dx\ntanddy\nt, can be seen\nas image channels (shown in Fig. 2), well suited to recognition using a convolutional network. To\nrepresent the motion across a sequence of frames, we stack the Ô¨Çow channels dx;y\ntofLconsecutive\nframes to form a total of 2Linput channels. More formally, let wandhbe the width and height\nof a video; a ConvNet input volume I\u001c2Rw\u0002h\u00022Lfor an arbitrary frame \u001cis then constructed as\nfollows:\nI\u001c(u;v;2k\u00001) =dx\n\u001c+k\u00001(u;v); (1)\nI\u001c(u;v;2k) =dy\n\u001c+k\u00001(u;v); u = [1;w];v= [1;h];k= [1;L]:\nFor an arbitrary point (u;v), the channels I\u001c(u;v;c );c= [1; 2L]encode the motion at that point\nover a sequence of Lframes (as illustrated in Fig. 3-left).\n3Trajectory stacking. An alternative motion representation, inspired by the trajectory-based de-\nscriptors [29], replaces the optical Ô¨Çow, sampled at the same locations across several frames, with\nthe Ô¨Çow, sampled along the motion trajectories. In this case, the input volume I\u001c, corresponding to\na frame\u001c, takes the following form:\nI\u001c(u;v;2k\u00001) =dx\n\u001c+k\u00001(pk); (2)\nI\u001c(u;v;2k) =dy\n\u001c+k\u00001(pk); u = [1;w];v= [1;h];k= [1;L]:\nwhere pkis thek-th point along the trajectory, which starts at the location (u;v)in the frame \u001cand\nis deÔ¨Åned by the following recurrence relation:\np1= (u;v); pk=pk\u00001+d\u001c+k\u00002(pk\u00001); k> 1:\nCompared to the input volume representation (1), where the channels I\u001c(u;v;c )store the displace-\nment vectors at the locations (u;v), the input volume (2) stores the vectors sampled at the locations\npkalong the trajectory (as illustrated in Fig. 3-right).\ninput volume channels  \n                 at point  \ninput volume channels  \n                 at point  \nFigure 3: ConvNet input derivation from the multi-frame optical Ô¨Çow. Left: optical Ô¨Çow stack-\ning (1) samples the displacement vectors dat the same location in multiple frames. Right: trajectory\nstacking (2) samples the vectors along the trajectory. The frames and the corresponding displace-\nment vectors are shown with the same colour.\nBi-directional optical Ô¨Çow. Optical Ô¨Çow representations (1) and (2) deal with the forward optical\nÔ¨Çow, i.e. the displacement Ô¨Åeld dtof the frame tspeciÔ¨Åes the location of its pixels in the following\nframet+ 1. It is natural to consider an extension to a bi-directional optical Ô¨Çow, which can be\nobtained by computing an additional set of displacement Ô¨Åelds in the opposite direction. We then\nconstruct an input volume I\u001cby stackingL=2forward Ô¨Çows between frames \u001cand\u001c+L=2andL=2\nbackward Ô¨Çows between frames \u001c\u0000L=2and\u001c. The inputI\u001cthus has the same number of channels\n(2L) as before. The Ô¨Çows can be represented using either of the two methods (1) and (2).\nMean Ô¨Çow subtraction. It is generally beneÔ¨Åcial to perform zero-centering of the network input,\nas it allows the model to better exploit the rectiÔ¨Åcation non-linearities. In our case, the displacement\nvector Ô¨Åeld components can take on both positive and negative values, and are naturally centered in\nthe sense that across a large variety of motions, the movement in one direction is as probable as the\nmovement in the opposite one. However, given a pair of frames, the optical Ô¨Çow between them can\nbe dominated by a particular displacement, e.g. caused by the camera movement. The importance\nof camera motion compensation has been previously highlighted in [10, 26], where a global motion\ncomponent was estimated and subtracted from the dense Ô¨Çow. In our case, we consider a simpler\napproach: from each displacement Ô¨Åeld dwe subtract its mean vector.\nArchitecture. Above we have described different ways of combining multiple optical Ô¨Çow displace-\nment Ô¨Åelds into a single volume I\u001c2Rw\u0002h\u00022L. Considering that a ConvNet requires a Ô¨Åxed-size\ninput, we sample a 224\u0002224\u00022Lsub-volume from I\u001cand pass it to the net as input. The hid-\nden layers conÔ¨Åguration remains largely the same as that used in the spatial net, and is illustrated\nin Fig. 1. Testing is similar to the spatial ConvNet, and is described in detail in Sect. 5.\n3.2 Relation of the temporal ConvNet architecture to previous representations\nIn this section, we put our temporal ConvNet architecture in the context of prior art, drawing con-\nnections to the video representations, reviewed in Sect. 1.1. Methods based on feature encod-\nings [17, 29] typically combine several spatio-temporal local features. Such features are computed\nfrom the optical Ô¨Çow and are thus generalised by our temporal ConvNet. Indeed, the HOF and MBH\n4local descriptors are based on the histograms of orientations of optical Ô¨Çow or its gradient, which\ncan be obtained from the displacement Ô¨Åeld input (1) using a single convolutional layer (containing\norientation-sensitive Ô¨Ålters), followed by the rectiÔ¨Åcation and pooling layers. The kinematic features\nof [10] (divergence, curl and shear) are also computed from the optical Ô¨Çow gradient, and, again, can\nbe captured by our convolutional model. Finally, the trajectory feature [29] is computed by stack-\ning the displacement vectors along the trajectory, which corresponds to the trajectory stacking (2).\nIn Sect. 3.3 we visualise the convolutional Ô¨Ålters, learnt in the Ô¨Årst layer of the temporal network.\nThis provides further evidence that our representation generalises hand-crafted features.\nAs far as the deep networks are concerned, a two-stream video classiÔ¨Åcation architecture of [16]\ncontains two HMAX models which are hand-crafted and less deep than our discriminatively trained\nConvNets, which can be seen as a learnable generalisation of HMAX. The convolutional models\nof [12, 14] do not decouple spatial and temporal recognition streams, and rely on the motion-\nsensitive convolutional Ô¨Ålters, learnt from the data. In our case, motion is explicitly represented\nusing the optical Ô¨Çow displacement Ô¨Åeld, computed based on the assumptions of constancy of the\nintensity and smoothness of the Ô¨Çow. Incorporating such assumptions into a ConvNet framework\nmight be able to boost the performance of end-to-end ConvNet-based methods, and is an interesting\ndirection for future research.\n3.3 Visualisation of learnt convolutional Ô¨Ålters\n96 1 2 \nflow 1  \nflow 10  \nflow 1  \nflow 10  conv. filters on  \nvertical flow  \ncomponents  ùëëùë¶ \n conv. fliters  on \nhorizontal flow  \ncomponents  ùëëùë• \n \ntemporal  \nderivative  \nspatial  \nderivative  \nFigure 4: First-layer convolutional Ô¨Ålters learnt on 10 stacked optical Ô¨Çows. The visualisation\nis split into 96 columns and 20 rows: each column corresponds to a Ô¨Ålter, each row ‚Äì to an input\nchannel.\nIn Fig. 4 we visualise the convolutional Ô¨Ålters from the Ô¨Årst layer of the temporal ConvNet, trained\non the UCF-101 dataset. Each of the 96Ô¨Ålters has a spatial receptive Ô¨Åeld of 7\u00027pixels, and spans\n20 input channels, corresponding to the horizontal ( dx) and vertical ( dy) components of 10stacked\noptical Ô¨Çow displacement Ô¨Åelds d.\nAs can be seen, some Ô¨Ålters compute spatial derivatives of the optical Ô¨Çow, capturing how mo-\ntion changes with image location, which generalises derivative-based hand-crafted descriptors (e.g.\nMBH). Other Ô¨Ålters compute temporal derivatives, capturing changes in motion over time.\n4 Multi-task learning\nUnlike the spatial stream ConvNet, which can be pre-trained on a large still image classiÔ¨Åcation\ndataset (such as ImageNet), the temporal ConvNet needs to be trained on video data ‚Äì and the\navailable datasets for video action classiÔ¨Åcation are still rather small. In our experiments (Sect. 6),\ntraining is performed on the UCF-101 and HMDB-51 datasets, which have only: 9.5K and 3.7K\nvideos respectively. To decrease over-Ô¨Åtting, one could consider combining the two datasets into\none; this, however, is not straightforward due to the intersection between the sets of classes. One\noption (which we evaluate later) is to only add the images from the classes, which do not appear in\nthe original dataset. This, however, requires manual search for such classes and limits the amount\nof additional training data.\nA more principled way of combining several datasets is based on multi-task learning [5]. Its aim\nis to learn a (video) representation, which is applicable not only to the task in question (such as\nHMDB-51 classiÔ¨Åcation), but also to other tasks (e.g. UCF-101 classiÔ¨Åcation). Additional tasks act\nas a regulariser, and allow for the exploitation of additional training data. In our case, a ConvNet\narchitecture is modiÔ¨Åed so that it has twosoftmax classiÔ¨Åcation layers on top of the last fully-\n5connected layer: one softmax layer computes HMDB-51 classiÔ¨Åcation scores, the other one ‚Äì the\nUCF-101 scores. Each of the layers is equipped with its own loss function, which operates only on\nthe videos, coming from the respective dataset. The overall training loss is computed as the sum of\nthe individual tasks‚Äô losses, and the network weight derivatives can be found by back-propagation.\n5 Implementation details\nConvNets conÔ¨Åguration. The layer conÔ¨Åguration of our spatial and temporal ConvNets is schemat-\nically shown in Fig. 1. It corresponds to CNN-M-2048 architecture of [3] and is similar to the\nnetwork of [31]. All hidden weight layers use the rectiÔ¨Åcation (ReLU) activation function; max-\npooling is performed over 3\u00023spatial windows with stride 2; local response normalisation uses the\nsame settings as [15]. The only difference between spatial and temporal ConvNet conÔ¨Ågurations is\nthat we removed the second normalisation layer from the latter to reduce memory consumption.\nTraining. The training procedure can be seen as an adaptation of that of [15] to video frames, and\nis generally the same for both spatial and temporal nets. The network weights are learnt using the\nmini-batch stochastic gradient descent with momentum (set to 0.9). At each iteration, a mini-batch\nof 256 samples is constructed by sampling 256 training videos (uniformly across the classes), from\neach of which a single frame is randomly selected. In spatial net training, a 224\u0002224sub-image is\nrandomly cropped from the selected frame; it then undergoes random horizontal Ô¨Çipping and RGB\njittering. The videos are rescaled beforehand, so that the smallest side of the frame equals 256. We\nnote that unlike [15], the sub-image is sampled from the whole frame, not just its 256\u0002256center.\nIn the temporal net training, we compute an optical Ô¨Çow volume Ifor the selected training frame as\ndescribed in Sect. 3. From that volume, a Ô¨Åxed-size 224\u0002224\u00022Linput is randomly cropped and\nÔ¨Çipped. The learning rate is initially set to 10\u00002, and then decreased according to a Ô¨Åxed schedule,\nwhich is kept the same for all training sets. Namely, when training a ConvNet from scratch, the rate\nis changed to 10\u00003after 50K iterations, then to 10\u00004after 70K iterations, and training is stopped\nafter 80K iterations. In the Ô¨Åne-tuning scenario, the rate is changed to 10\u00003after 14K iterations, and\ntraining stopped after 20K iterations.\nTesting. At test time, given a video, we sample a Ô¨Åxed number of frames ( 25in our experiments)\nwith equal temporal spacing between them. From each of the frames we then obtain 10 ConvNet\ninputs [15] by cropping and Ô¨Çipping four corners and the center of the frame. The class scores for the\nwhole video are then obtained by averaging the scores across the sampled frames and crops therein.\nPre-training on ImageNet ILSVRC-2012. When pre-training the spatial ConvNet, we use the\nsame training and test data augmentation as described above (cropping, Ô¨Çipping, RGB jittering).\nThis yields 13:5%top-5 error on ILSVRC-2012 validation set, which compares favourably to 16:0%\nreported in [31] for a similar network. We believe that the main reason for the improvement is\nsampling of ConvNet inputs from the whole image, rather than just its center.\nMulti-GPU training. Our implementation is derived from the publicly available Caffe toolbox [13],\nbut contains a number of signiÔ¨Åcant modiÔ¨Åcations, including parallel training on multiple GPUs\ninstalled in a single system. We exploit the data parallelism, and split each SGD batch across several\nGPUs. Training a single temporal ConvNet takes 1 day on a system with 4 NVIDIA Titan cards,\nwhich constitutes a 3:2times speed-up over single-GPU training.\nOptical Ô¨Çow is computed using the off-the-shelf GPU implementation of [2] from the OpenCV\ntoolbox. In spite of the fast computation time ( 0:06s for a pair of frames), it would still introduce\na bottleneck if done on-the-Ô¨Çy, so we pre-computed the Ô¨Çow before training. To avoid storing\nthe displacement Ô¨Åelds as Ô¨Çoats, the horizontal and vertical components of the Ô¨Çow were linearly\nrescaled to a [0;255] range and compressed using JPEG (after decompression, the Ô¨Çow is rescaled\nback to its original range). This reduced the Ô¨Çow size for the UCF-101 dataset from 1.5TB to 27GB.\n6 Evaluation\nDatasets and evaluation protocol. The evaluation is performed on UCF-101 [24] and\nHMDB-51 [16] action recognition benchmarks, which are among the largest available annotated\nvideo datasets1. UCF-101 contains 13K videos (180 frames/video on average), annotated into 101\naction classes; HMDB-51 includes 6.8K videos of 51 actions. The evaluation protocol is the same\n1Very recently, [14] released the Sports-1M dataset of 1.1M automatically annotated YouTube sports videos.\nProcessing the dataset of such scale is very challenging, and we plan to address it in future work.\n6for both datasets: the organisers provide three splits into training and test data, and the performance\nis measured by the mean classiÔ¨Åcation accuracy across the splits. Each UCF-101 split contains 9.5K\ntraining videos; an HMDB-51 split contains 3.7K training videos. We begin by comparing different\narchitectures on the Ô¨Årst split of the UCF-101 dataset. For comparison with the state of the art, we\nfollow the standard evaluation protocol and report the average accuracy over three splits on both\nUCF-101 and HMDB-51.\nSpatial ConvNets. First, we measure the performance of the spatial stream ConvNet. Three sce-\nnarios are considered: (i) training from scratch on UCF-101, (ii) pre-training on ILSVRC-2012\nfollowed by Ô¨Åne-tuning on UCF-101, (iii) keeping the pre-trained network Ô¨Åxed and only training\nthe last (classiÔ¨Åcation) layer. For each of the settings, we experiment with setting the dropout regu-\nlarisation ratio to 0:5or to 0:9. From the results, presented in Table 1a, it is clear that training the\nConvNet solely on the UCF-101 dataset leads to over-Ô¨Åtting (even with high dropout), and is inferior\nto pre-training on a large ILSVRC-2012 dataset. Interestingly, Ô¨Åne-tuning the whole network gives\nonly marginal improvement over training the last layer only. In the latter setting, higher dropout\nover-regularises learning and leads to worse accuracy. In the following experiments we opted for\ntraining the last layer on top of a pre-trained ConvNet.\nTable 1: Individual ConvNets accuracy on UCF-101 (split 1).\n(a)Spatial ConvNet.\nTraining settingDropout ratio\n0:5 0:9\nFrom scratch 42.5% 52.3%\nPre-trained + Ô¨Åne-tuning 70.8% 72.8%\nPre-trained + last layer 72.7% 59.9%(b)Temporal ConvNet.\nInput conÔ¨ÅgurationMean subtraction\noff on\nSingle-frame optical Ô¨Çow ( L= 1) - 73.9%\nOptical Ô¨Çow stacking (1) ( L= 5) - 80.4%\nOptical Ô¨Çow stacking (1) ( L= 10 ) 79.9% 81.0%\nTrajectory stacking (2)( L= 10 ) 79.6% 80.2%\nOptical Ô¨Çow stacking (1)( L= 10 ), bi-dir. - 81.2%\nTemporal ConvNets. Having evaluated spatial ConvNet variants, we now turn to the temporal\nConvNet architectures, and assess the effect of the input conÔ¨Ågurations, described in Sect. 3.1. In\nparticular, we measure the effect of: using multiple ( L=f5;10g) stacked optical Ô¨Çows; trajectory\nstacking; mean displacement subtraction; using the bi-directional optical Ô¨Çow. The architectures\nare trained on the UCF-101 dataset from scratch, so we used an aggressive dropout ratio of 0:9to\nhelp improve generalisation. The results are shown in Table 1b. First, we can conclude that stacking\nmultiple (L>1) displacement Ô¨Åelds in the input is highly beneÔ¨Åcial, as it provides the network with\nlong-term motion information, which is more discriminative than the Ô¨Çow between a pair of frames\n(L= 1setting). Increasing the number of input Ô¨Çows from 5to10leads to a smaller improvement,\nso we keptLÔ¨Åxed to 10in the following experiments. Second, we Ô¨Ånd that mean subtraction is\nhelpful, as it reduces the effect of global motion between the frames. We use it in the following\nexperiments as default. The difference between different stacking techniques is marginal; it turns\nout that optical Ô¨Çow stacking performs better than trajectory stacking, and using the bi-directional\noptical Ô¨Çow is only slightly better than a uni-directional forward Ô¨Çow. Finally, we note that temporal\nConvNets signiÔ¨Åcantly outperform the spatial ConvNets (Table 1a), which conÔ¨Årms the importance\nof motion information for action recognition.\nWe also implemented the ‚Äúslow fusion‚Äù architecture of [14], which amounts to applying a ConvNet\nto a stack of RGB frames ( 11frames in our case). When trained from scratch on UCF-101, it\nachieved 56:4%accuracy, which is better than a single-frame architecture trained from scratch\n(52:3%), but is still far off the network trained from scratch on optical Ô¨Çow. This shows that while\nmulti-frame information is important, it is also important to present it to a ConvNet in an appropriate\nmanner.\nMulti-task learning of temporal ConvNets. Training temporal ConvNets on UCF-101 is challeng-\ning due to the small size of the training set. An even bigger challenge is to train the ConvNet on\nHMDB-51, where each training split is 2:6times smaller than that of UCF-101. Here we evaluate\ndifferent options for increasing the effective training set size of HMDB-51: (i) Ô¨Åne-tuning a temporal\nnetwork pre-trained on UCF-101; (ii) adding 78classes from UCF-101, which are manually selected\nso that there is no intersection between these classes and the native HMDB-51 classes; (iii) using the\nmulti-task formulation (Sect. 4) to learn a video representation, shared between the UCF-101 and\nHMDB-51 classiÔ¨Åcation tasks. The results are reported in Table 2. As expected, it is beneÔ¨Åcial to\n7Table 2: Temporal ConvNet accuracy on HMDB-51 (split 1 with additional training data).\nTraining setting Accuracy\nTraining on HMDB-51 without additional data 46.6%\nFine-tuning a ConvNet, pre-trained on UCF-101 49.0%\nTraining on HMDB-51 with classes added from UCF-101 52.8%\nMulti-task learning on HMDB-51 and UCF-101 55.4%\nutilise full (all splits combined) UCF-101 data for training (either explicitly by borrowing images, or\nimplicitly by pre-training). Multi-task learning performs the best, as it allows the training procedure\nto exploit all available training data.\nWe have also experimented with multi-task learning on the UCF-101 dataset, by training a network\nto classify both the full HMDB-51 data (all splits combined) and the UCF-101 data (a single split).\nOn the Ô¨Årst split of UCF-101, the accuracy was measured to be 81.5%, which improves on 81:0%\nachieved using the same settings, but without the additional HMDB classiÔ¨Åcation task (Table 1b).\nTwo-stream ConvNets. Here we evaluate the complete two-stream model, which combines the\ntwo recognition streams. One way of combining the networks would be to train a joint stack of\nfully-connected layers on top of full6 or full7 layers of the two nets. This, however, was not feasible\nin our case due to over-Ô¨Åtting. We therefore fused the softmax scores using either averaging or\na linear SVM. From Table 3 we conclude that: (i) temporal and spatial recognition streams are\ncomplementary, as their fusion signiÔ¨Åcantly improves on both (6%over temporal and 14% over\nspatial nets); (ii) SVM-based fusion of softmax scores outperforms fusion by averaging; (iii) using\nbi-directional Ô¨Çow is not beneÔ¨Åcial in the case of ConvNet fusion; (iv) temporal ConvNet, trained\nusing multi-task learning, performs the best both alone and when fused with a spatial net.\nTable 3: Two-stream ConvNet accuracy on UCF-101 (split 1).\nSpatial ConvNet Temporal ConvNet Fusion Method Accuracy\nPre-trained + last layer bi-directional averaging 85.6%\nPre-trained + last layer uni-directional averaging 85.9%\nPre-trained + last layer uni-directional, multi-task averaging 86.2%\nPre-trained + last layer uni-directional, multi-task SVM 87.0%\nComparison with the state of the art. We conclude the experimental evaluation with the com-\nparison against the state of the art on three splits of UCF-101 and HMDB-51. For that we used a\nspatial net, pre-trained on ILSVRC, with the last layer trained on UCF or HMDB. The temporal\nnet was trained on UCF and HMDB using multi-task learning, and the input was computed using\nuni-directional optical Ô¨Çow stacking with mean subtraction. The softmax scores of the two nets were\ncombined using averaging or SVM. As can be seen from Table 4, both our spatial and temporal nets\nalone outperform the deep architectures of [14, 16] by a large margin. The combination of the two\nnets further improves the results (in line with the single-split experiments above), and is comparable\nto the very recent state-of-the-art hand-crafted models [20, 21, 26].\nTable 4: Mean accuracy (over three splits) on UCF-101 and HMDB-51.\nMethod UCF-101 HMDB-51\nImproved dense trajectories (IDT) [26, 27] 85.9% 57.2%\nIDT with higher-dimensional encodings [20] 87.9% 61.1%\nIDT with stacked Fisher encoding [21] (based on Deep Fisher Net [23]) - 66.8%\nSpatio-temporal HMAX network [11, 16] - 22.8%\n‚ÄúSlow fusion‚Äù spatio-temporal ConvNet [14] 65.4% -\nSpatial stream ConvNet 73.0% 40.5%\nTemporal stream ConvNet 83.7% 54.6%\nTwo-stream model (fusion by averaging) 86.9% 58.0%\nTwo-stream model (fusion by SVM) 88.0% 59.4%\nConfusion matrix and per-class recall for UCF-101 classiÔ¨Åcation. In Fig. 5 we show the confu-\nsion matrix for UCF-101 classiÔ¨Åcation using our two-stream model, which achieves 87:0%accuracy\non the Ô¨Årst dataset split (the last row of Table 3). We also visualise the corresponding per-class recall\nin Fig. 6.\n8The worst class recall corresponds to Hammering class, which is confused with HeadMassage and\nBrushingTeeth classes. We found that this is due to two reasons. First, the spatial ConvNet confuses\nHammering with HeadMassage , which can be caused by the signiÔ¨Åcant presence of human faces\nin both classes. Second, the temporal ConvNet confuses Hammering with BrushingTeeth , as both\nactions contain recurring motion patterns (hand moving up and down).\nFigure 5: Confusion matrix of a two-stream model on the Ô¨Årst split of UCF-101.\n7 Conclusions and directions for improvement\nWe proposed a deep video classiÔ¨Åcation model with competitive performance, which incorporates\nseparate spatial and temporal recognition streams based on ConvNets. Currently it appears that\ntraining a temporal ConvNet on optical Ô¨Çow (as here) is signiÔ¨Åcantly better than training on raw\nstacked frames [14]. The latter is probably too challenging, and might require architectural changes\n(for example, a combination with the deep matching approach of [30]). Despite using optical Ô¨Çow\nas input, our temporal model does not require signiÔ¨Åcant hand-crafting, since the Ô¨Çow is computed\nusing a method based on the generic assumptions of constancy and smoothness.\nAs we have shown, extra training data is beneÔ¨Åcial for our temporal ConvNet, so we are planning to\ntrain it on large video datasets, such as the recently released collection of [14]. This, however, poses\na signiÔ¨Åcant challenge on its own due to the gigantic amount of training data (multiple TBs).\nThere still remain some essential ingredients of the state-of-the-art shallow representation [26],\nwhich are missed in our current architecture. The most prominent one is local feature pooling\nover spatio-temporal tubes, centered at the trajectories. Even though the input (2) captures the opti-\ncal Ô¨Çow along the trajectories, the spatial pooling in our network does not take the trajectories into\naccount. Another potential area of improvement is explicit handling of camera motion, which in our\ncase is compensated by mean displacement subtraction.\n9Figure 6: Per-class recall of a two-stream model on the Ô¨Årst split of UCF-101.\nAcknowledgements\nThis work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support\nof NVIDIA Corporation with the donation of the GPUs used for this research.\nReferences\n[1] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge (ILSVRC), 2010. URL\nhttp://www.image-net.org/challenges/LSVRC/2010/ .\n[2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical Ô¨Çow estimation based on a\ntheory for warping. In Proc. ECCV , pages 25‚Äì36, 2004.\n[3] K. ChatÔ¨Åeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep\ninto convolutional nets. In Proc. BMVC. , 2014.\n[4] B. Chen, J. A. Ting, B. Marlin, and N. de Freitas. Deep learning of invariant spatio-temporal features\nfrom video. In NIPS Deep Learning and Unsupervised Feature Learning Workshop , 2010.\n[5] R. Collobert and J. Weston. A uniÔ¨Åed architecture for natural language processing: deep neural networks\nwith multitask learning. In Proc. ICML , pages 160‚Äì167, 2008.\n[6] K. Crammer and Y . Singer. On the algorithmic implementation of multiclass kernel-based vector ma-\nchines. JMLR , 2:265‚Äì292, 2001.\n[7] N. Dalal and B Triggs. Histogram of Oriented Gradients for Human Detection. In Proc. CVPR , volume 2,\npages 886‚Äì893, 2005.\n[8] N. Dalal, B. Triggs, and C. Schmid. Human detection using oriented histograms of Ô¨Çow and appearance.\nInProc. ECCV , pages 428‚Äì441, 2006.\n[9] M. A. Goodale and A. D. Milner. Separate visual pathways for perception and action. Trends in Neuro-\nsciences , 15(1):20‚Äì25, 1992.\n[10] M. Jain, H. Jegou, and P. Bouthemy. Better exploiting motion for better action recognition. In Proc.\nCVPR , pages 2555‚Äì2562, 2013.\n[11] H. Jhuang, T. Serre, L. Wolf, and T. Poggio. A biologically inspired system for action recognition. In\nProc. ICCV , pages 1‚Äì8, 2007.\n[12] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural networks for human action recognition. IEEE\nPAMI , 35(1):221‚Äì231, 2013.\n[13] Y . Jia. Caffe: An open source convolutional architecture for fast feature embedding. http://caffe.\nberkeleyvision.org/ , 2013.\n[14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classiÔ¨Å-\ncation with convolutional neural networks. In Proc. CVPR , 2014.\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiÔ¨Åcation with deep convolutional neural\nnetworks. In NIPS , pages 1106‚Äì1114, 2012.\n10[16] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A large video database for human\nmotion recognition. In Proc. ICCV , pages 2556‚Äì2563, 2011.\n[17] I. Laptev, M. Marsza≈Çek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In\nProc. CVPR , 2008.\n[18] Q. V . Le, W. Y . Zou, S. Y . Yeung, and A. Y . Ng. Learning hierarchical invariant spatio-temporal features\nfor action recognition with independent subspace analysis. In Proc. CVPR , pages 3361‚Äì3368, 2011.\n[19] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backprop-\nagation applied to handwritten zip code recognition. Neural Computation , 1(4):541‚Äì551, 1989.\n[20] X. Peng, L. Wang, X. Wang, and Y . Qiao. Bag of visual words and fusion methods for action recognition:\nComprehensive study and good practice. CoRR , abs/1405.4506, 2014.\n[21] X. Peng, C. Zou, Y . Qiao, and Q. Peng. Action recognition with stacked Ô¨Åsher vectors. In Proc. ECCV ,\npages 581‚Äì595, 2014.\n[22] F. Perronnin, J. S ¬¥anchez, and T. Mensink. Improving the Fisher kernel for large-scale image classiÔ¨Åcation.\nInProc. ECCV , 2010.\n[23] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep Fisher networks for large-scale image classiÔ¨Åcation.\nInNIPS , 2013.\n[24] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of 101 human actions classes from videos in\nthe wild. CoRR , abs/1212.0402, 2012.\n[25] G. W. Taylor, R. Fergus, Y . LeCun, and C. Bregler. Convolutional learning of spatio-temporal features.\nInProc. ECCV , pages 140‚Äì153, 2010.\n[26] H. Wang and C. Schmid. Action recognition with improved trajectories. In Proc. ICCV , pages 3551‚Äì3558,\n2013.\n[27] H. Wang and C. Schmid. LEAR-INRIA submission for the THUMOS workshop. In ICCV Workshop on\nAction Recognition with a Large Number of Classes , 2013.\n[28] H. Wang, M. M. Ullah, A. Kl ¬®aser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features\nfor action recognition. In Proc. BMVC. , pages 1‚Äì11, 2009.\n[29] H. Wang, A. Kl ¬®aser, C. Schmid, and C.-L. Liu. Action recognition by dense trajectories. In Proc. CVPR ,\npages 3169‚Äì3176, 2011.\n[30] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. DeepFlow: Large displacement optical Ô¨Çow\nwith deep matching. In Proc. ICCV , pages 1385‚Äì1392, 2013.\n[31] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. CoRR , abs/1311.2901,\n2013.\n11"
  }
}